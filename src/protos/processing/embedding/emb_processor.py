from ..processing.grn.grn_processor import *import numpy as npimport torchimport osimport picklefrom typing import Listfrom tqdm import tqdmimport plotly.graph_objects as gofrom sklearn.manifold import TSNEimport esmdef get_model_and_tokenizer(model_name):    from ankh import load_base_model, load_large_model    if model_name == 'ankh_base':        model, tokenizer = load_base_model()        model.eval()        emb_size = 768        return model, tokenizer, emb_size    elif model_name == 'ankh_large':        model, tokenizer = load_large_model()        model.eval()        emb_size = 768 * 2        return model, tokenizer, emb_size    else:        return None, None, Noneimport loggingimport tracebacklogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)class EMBModelManager:    def __init__(self, model_name='ankh_base', device='cuda'):        self.device = device        self.model, self.tokenizer, self.emb_size = None, None, None        self.initialize_model(model_name)    def initialize_model(self, model_name='ankh_base'):        logger.info(f"Initializing model: {model_name}")        logger.info(f"CUDA available: {torch.cuda.is_available()}")        logger.info(f"Using device: {self.device}")        if torch.cuda.is_available():            logger.info(f"CUDA device: {torch.cuda.get_device_name(0)}")            logger.info(f"CUDA memory allocated: {torch.cuda.memory_allocated(0)}")            logger.info(f"CUDA memory reserved: {torch.cuda.memory_reserved(0)}")        if model_name == 'esm2':            model_name = 'esm2_t36_3B_UR50D'  # this is not the full model, but it's the largest working for now            logger.info(f"Loading ESM2 model: {model_name}")            model, tokenizer = esm.pretrained.load_model_and_alphabet(model_name)            emb_size = 2560        elif model_name == 'ankh_base':            from ankh import load_base_model            model, tokenizer = load_base_model()            emb_size = 768        elif model_name == 'ankh_large':            from ankh import load_large_model            model, tokenizer = load_large_model()            emb_size = 768 * 2        else:            raise ValueError(f"Model {model_name} is not supported.")        model.eval()  # Ensure the model is in evaluation mode        self.model = model.to(self.device)        self.tokenizer = tokenizer        self.emb_size = emb_size    def to_device(self, device):        self.device = device        self.model.to(device)class EMBProcessor:    def __init__(self,                 model_manager=None,                 dataset: str = None,                 path='data/embeddings/',                 device='cpu'):        if model_manager is None:            self.model_manager = None        else:            self.model_manager = model_manager            self.model_manager.to_device(device)        self.emb_dict = {}        self.ids = []        self.device = device        self.dataset_path = path        if dataset != None:            self.load_dataset(dataset)    def emb_grnp(self, grnp: GRNProcessor, overwrite: bool = True):        for idx, seq in tqdm(grnp.get_seq_dict().items()):            if idx not in self.emb_dict.keys() or overwrite:                model_input = [list(seq)]                ids = self.model_manager.tokenizer.batch_encode_plus(model_input,                                                       add_special_tokens=True,                                                       padding=True,                                                       is_split_into_words=True,                                                       return_tensors="pt")['input_ids'].to(self.device)                output = self.model_manager.model(input_ids=ids,                                                  output_attentions=False)                self.emb_dict[idx] = output.last_hidden_state.detach().cpu().numpy().squeeze(0)[:-1, :]    def emb_seq_dict(self, seq_dict: dict, overwrite: bool = True):        try:            for idx, seq in seq_dict.items():                if idx not in self.emb_dict.keys() or overwrite:                    if isinstance(self.model_manager.model, esm.pretrained.ESM2):                        logger.info(f"Processing sequence {idx} with ESM2")                        batch_converter = self.model_manager.tokenizer.get_batch_converter()                        batch_labels, batch_strs, batch_tokens = batch_converter([(idx, seq)])                        with torch.no_grad():                            results = self.model_manager.model(batch_tokens.to(self.device),                                                               repr_layers=[self.model_manager.model.num_layers],                                                               return_contacts=False)                        token_representations = results["representations"][self.model_manager.model.num_layers]                        self.emb_dict[idx] = token_representations[0, 1:len(seq) + 1].cpu().numpy()                    else:                        # Existing processing for other models                        model_input = [list(seq)]                        ids = self.model_manager.tokenizer.batch_encode_plus(model_input,                                                                             add_special_tokens=True,                                                                             padding=True,                                                                             is_split_into_words=True,                                                                             return_tensors="pt")['input_ids'].to(                            self.device)                        output = self.model_manager.model(input_ids=ids,                                                          output_attentions=False)                        self.emb_dict[idx] = output.last_hidden_state.detach().cpu().numpy().squeeze(0)[:-1, :]        except Exception as e:            logger.error(f"Error in emb_seq_dict: {str(e)}")            logger.error(traceback.format_exc())            raise    def save_dataset(self, dataset_name: str):        dataset_full_path = os.path.join(self.dataset_path, f"{dataset_name}.pkl")        with open(dataset_full_path, 'wb') as f:            pickle.dump(self.emb_dict, f, protocol=pickle.HIGHEST_PROTOCOL)    def load_dataset(self, dataset_name: str):        dataset_full_path = os.path.join(self.dataset_path, f"{dataset_name}.pkl")        if not os.path.exists(dataset_full_path):            raise FileNotFoundError(f"Dataset {dataset_name} not found in {self.dataset_path}")        with open(dataset_full_path, 'rb') as f:            self.emb_dict = pickle.load(f)            self.ids = list(self.emb_dict.keys())    def list_available_datasets(self) -> List[str]:        if not os.path.exists(self.dataset_path):            print(f"No dataset directory found at {self.dataset_path}")            return []        files = os.listdir(self.dataset_path)        datasets = [file for file in files if file.endswith('.pkl')]        return datasets    def filter_by_ids(self, ids: list[str]):        self.emb_dict = {k: v for k, v in self.emb_dict.items() if k in ids}        self.ids = ids    def get_grn_embeddings(self, grn_list, grnp, inplace=False, fill_zero=True):        updated_emb_dict = {}        for seq_id, sequence in grnp.get_seq_dict().items():            if seq_id in self.emb_dict:                selected_embeddings = []                for grn in grn_list:                    residue_info = grnp.data.loc[seq_id, grn]                    if residue_info != '-':                        residue_number = int(''.join(filter(str.isdigit, residue_info))) - 1                        if residue_number < len(self.emb_dict[seq_id]):                            embedding = self.emb_dict[seq_id][residue_number, :]                            selected_embeddings.append(embedding)                        else:                            # If the residue number is out of bounds, add a zero vector                            selected_embeddings.append(np.zeros(self.model_manager.emb_size))                    elif fill_zero:                        # If there is no residue info for a GRN, add a zero vector                        selected_embeddings.append(np.zeros(self.model_manager.emb_size))                if selected_embeddings:                    # Stack the selected embeddings or zero vectors                    updated_emb_dict[seq_id] = np.stack(selected_embeddings, axis=0)        if inplace:            self.emb_dict = updated_emb_dict        else:            return updated_emb_dict    def map_embeddings_to_grns(self, grn_list, grnp):        updated_emb_dict = {}        for seq_id, sequence in grnp.get_seq_dict().items():            if seq_id in self.emb_dict:                selected_embeddings = []                for grn in grn_list:                    residue_info = grnp.data.loc[seq_id, grn]                    if residue_info != '-':                        residue_number = int(''.join(filter(str.isdigit, residue_info))) - 1                        embedding = self.emb_dict[seq_id][residue_number, :]                        selected_embeddings.append(embedding)                if selected_embeddings:                    updated_emb_dict[seq_id] = np.stack(selected_embeddings, axis=0)[np.newaxis, :]        return updated_emb_dict    def to_numpy(self):        for uen, emb in self.emb_dict.items():            # Ensure the tensor is on CPU before converting to numpy array            self.emb_dict[uen] = emb.cpu().numpy()    def to_tensor(self):        for uen, emb in self.emb_dict.items():            self.emb_dict[uen] = torch.tensor(emb, device=self.device)    def aggr_embeddings(self, embeddings=None, operation='sum'):        if embeddings is None:            embeddings = self.emb_dict        aggregated_embeddings = {}        for key, emb in embeddings.items():            if operation == 'sum':                aggregated_embeddings[key] = np.sum(emb, axis=0)            elif operation == 'mean':                aggregated_embeddings[key] = np.mean(emb, axis=0)            elif operation == 'max':                aggregated_embeddings[key] = np.max(emb, axis=0)            elif operation == 'min':                aggregated_embeddings[key] = np.min(emb, axis=0)        return aggregated_embeddings    def load_and_merge_datasets(self, dataset_names):        merged_dict = {}        for dataset_name in dataset_names:            dataset_full_path = os.path.join(self.dataset_path, f"{dataset_name}.pkl")            if not os.path.exists(dataset_full_path):                raise FileNotFoundError(f"Dataset {dataset_name} not found in {self.dataset_path}")            with open(dataset_full_path, 'rb') as f:                dataset_dict = pickle.load(f)                merged_dict.update(dataset_dict)        self.emb_dict = merged_dict        self.ids = list(self.emb_dict.keys())def tsne_visualization(embeddings, labels=None, emb_ids=None):    # Convert embeddings to a list for TSNE processing    emb_list = np.array([emb for emb in embeddings.values()])    # Check if the embeddings are already flat    if emb_list.ndim >= 2:        print("Warning: Embeddings have more than 2 dimensions and will be averaged for visualization.")        emb_list = emb_list.mean(axis=1)  # Averaging if the embeddings are multi-dimensional    # t-SNE Dimensionality Reduction    tsne = TSNE(n_components=2, random_state=42)    tsne_results = tsne.fit_transform(emb_list)    # Prepare plot    fig = go.Figure()    if labels is not None:        # Ensure labels list matches the number of embeddings        if len(labels) != len(emb_list):            raise ValueError("Length of labels does not match the number of embeddings.")        # Map labels to colors if labels are provided        unique_labels = list(set(labels))        colors = [f'hsl({i / len(unique_labels) * 360}, 100%, 50%)' for i in range(len(unique_labels))]        label_to_color = {label: color for label, color in zip(unique_labels, colors)}        for label in unique_labels:            indices = [i for i, l in enumerate(labels) if l == label]            hovertexts = [emb_ids[i] if emb_ids else '' for i in indices] if emb_ids else None            fig.add_trace(go.Scatter(x=tsne_results[indices, 0], y=tsne_results[indices, 1],                                     mode='markers',                                     marker=dict(color=label_to_color[label]),                                     name=label,                                     text=hovertexts,                                     hoverinfo='text' if emb_ids else 'none'))    else:        # If no labels are provided, plot all points in blue        hovertexts = [emb_ids[i] if emb_ids else '' for i in range(len(emb_list))] if emb_ids else None        fig.add_trace(go.Scatter(x=tsne_results[:, 0], y=tsne_results[:, 1],                                 mode='markers',                                 marker=dict(color='blue'),                                 text=hovertexts,                                 hoverinfo='text' if emb_ids else 'none'))    # Update layout    fig.update_layout(title='t-SNE visualization of embeddings',                      xaxis_title='t-SNE 1',                      yaxis_title='t-SNE 2',                      legend_title="Labels",                      hovermode="closest")    fig.show()